### Model Optimization Pipeline

The model optimization pipeline consists of 4 core stages that follow the labeling phase: data augmentation, model training, distillation and quantization, and final model saving. These stages are implemented through a series of Python scripts that prepare the model for production use.

#### Input

The input to this pipeline will be the labeled images from the labeling pipeline, distillation images, and configuration files for augmentation, training, and distillation.

#### Process

**1. Data Augmentation (`augmentation.py`)**

![Overview of the augmentation pipeline](img/augmentation.png){width=20%}

This script increases the size and diversity of our dataset through common augmentation techniques (e.g., flipping, brightness/contrast adjustment, noise injection), as specified in the configuration file. These augmentations help reduce overfitting and improve generalization during model training.

**2. Model Training and Retraining (`train.py`)**

![Overview of the model training, distillation, and quantization pipeline](img/train-distill-quantize.png){width=40%}

This script fine-tunes the YOLOv8 base model using the augmented dataset and a configuration file, and produce a full trained model file (`.pt`). The full trained model will be registered for use in future labeling iterations and passed on to the distillation stage for optimization.

This training process will be repeated as new labeled data becomes available, enabling continuous model improvement over time.

**3. Model Distillation and Quantization (`distill_quantize.py`)**

This script optimizes the trained model in two stages. First, it performs **distillation** using the distillation images to produce a distilled model that is smaller and faster, while preserving accuracy. Then it applies **quantization** to further reduce model size and enhance deployment efficiency, enabling deployment on lightweight platforms.

**4. Deployment (`save_model.py`)**

This script finalizes the pipeline by registering all three model versions in the model registry. The quantized model will be marked as the current production version for deployment.

#### Output

The optimization pipeline will produce the following outputs:

- Full trained model (`full_trained_model.pt`)
- Distilled model (`distilled_model.pt`)
- Quantized model (`quantized_model.pt`)
- Updated configuration files for training and distillation
