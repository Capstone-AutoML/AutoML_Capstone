### Labeling Pipeline

The proposed semi-automated image labeling pipeline aims to efficiently annotate a large volume of unlabeled wildfire imagery. It integrates object detection, image segmentation, and human-in-the-loop validation to improve annotation accuracy while reducing manual effort.

#### Input
The input to the pipeline will be a collection of unlabeled images obtained from the projectâ€™s dataset.

#### Process
The pipeline consists of three main stages implemented in three corresponding Python scripts.

**1. Object Detection and Segmentation (`labeling.py`)**

A function in this script first applies **You Only Look Once (YOLO)**, a real-time object detection model, to generate initial bounding boxes and class labels (e.g., fire, smoke, vehicle) for each image [@yolo2020]. These predictions, including the class labels, are then passed to a second function that uses the **Segment Anything Model (SAM)**. SAM generates pixel-level segmentation masks for each labeled object using the corresponding bounding boxes as prompts [@sam2023].

![Overview of the proposed labeling pipeline combining YOLO for object detection, SAM for segmentation, and a matching process.](img/prelabelling_1.png){width=50%}

**2. Matching and Filtering (`matching.py`)**

A key challenge is reconciling the outputs from YOLO (bounding boxes) and SAM (segmentation masks). A matching function in the script evaluates each pair using either **Intersection over Union (IoU)**, which measures the overlap between bounding box and segmentation mask, or **mask containment**, which assesses how much of the segmentation area lies within the predicted bounding box. If the match score falls below a defined threshold, the case is flagged for manual review.

![Visual comparison of YOLO's bounding box and SAM's segmentation mask, highlighting the need for a matching criterion.](img/prelabelling_2.png){width=100%}

**3. Human-in-the-Loop Review (`human_intervention.py`)**

Flagged cases will be passed to **Label Studio**, an open-source annotation tool, for human verification [@labelstudio]. Reviewers will inspect and correct mismatches or low-confidence predictions, ensuring high labeling quality. This hybrid approach balances automation with manual oversight.

![Human-in-the-loop flow using Label Studio to validate flagged predictions.](img/prelabelling_3.png){width=70%}

![Label Studio interface displaying pre-labeled objects for reviewer validation.](img/prelabelling_4.png){width=70%}

#### Output
The final output of the labeling pipeline will be a set of high-quality annotated images. These will either be auto-labeled with high confidence or validated through human review and used to train downstream models.
